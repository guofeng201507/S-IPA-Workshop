{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NICF- Intelligent Process Automation (SF)\n",
    "\n",
    "https://www.iss.nus.edu.sg/executive-education/course/detail/intelligent-process-automation/artificial-intelligence\n",
    "\n",
    "# Workshop: Text Summarization\n",
    "\n",
    "### <span style=\"color:red\">Re-created with Python 3 version</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting goose3\n",
      "  Downloading https://files.pythonhosted.org/packages/29/0e/5d049211226268ebce83ae5c8c4f578af0f5f120b24de9542485efcfeda2/goose3-3.1.6-py3-none-any.whl (86kB)\n",
      "Collecting cssselect\n",
      "  Using cached https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\guofe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from goose3) (2.8.0)\n",
      "Collecting jieba\n",
      "  Using cached https://files.pythonhosted.org/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz\n",
      "Collecting requests\n",
      "  Downloading https://files.pythonhosted.org/packages/1a/70/1935c770cb3be6e3a8b78ced23d7e0f3b187f5cbfab4749523ed65d7c9b1/requests-2.23.0-py2.py3-none-any.whl (58kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading https://files.pythonhosted.org/packages/cb/a1/c698cf319e9cfed6b17376281bd0efc6bfc8465698f54170ef60a485ab5d/beautifulsoup4-4.8.2-py3-none-any.whl (106kB)\n",
      "Collecting nltk\n",
      "  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\guofe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from goose3) (7.0.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\guofe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from goose3) (4.4.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\guofe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from python-dateutil->goose3) (1.12.0)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading https://files.pythonhosted.org/packages/89/e3/afebe61c546d18fb1709a61bee788254b40e736cff7271c7de5de2dc4128/idna-2.9-py2.py3-none-any.whl (58kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/63/df50cac98ea0d5b006c55a399c3bf1db9da7b5a24de7890bc9cfd5dd9e99/certifi-2019.11.28-py2.py3-none-any.whl (156kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading https://files.pythonhosted.org/packages/e8/74/6e4f91745020f967d09332bb2b8b9b10090957334692eb88ea4afe91b77f/urllib3-1.25.8-py2.py3-none-any.whl (125kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\guofe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->goose3) (3.0.4)\n",
      "Collecting soupsieve>=1.2\n",
      "  Downloading https://files.pythonhosted.org/packages/05/cf/ea245e52f55823f19992447b008bcbb7f78efc5960d77f6c34b5b45b36dd/soupsieve-2.0-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: jieba, nltk\n",
      "  Building wheel for jieba (setup.py): started\n",
      "  Building wheel for jieba (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba: filename=jieba-0.42.1-cp37-none-any.whl size=19314482 sha256=152d48dc7708bb1b8b08ff0a0f290758ea01f7e903c264fadde834aa37872d63\n",
      "  Stored in directory: C:\\Users\\guofe\\AppData\\Local\\pip\\Cache\\wheels\\af\\e4\\8e\\5fdd61a6b45032936b8f9ae2044ab33e61577950ce8e0dec29\n",
      "  Building wheel for nltk (setup.py): started\n",
      "  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449913 sha256=c1ce55828c2ddb5f33eee1d9cc1298c01d98ec962e465904febcc7a752daef12\n",
      "  Stored in directory: C:\\Users\\guofe\\AppData\\Local\\pip\\Cache\\wheels\\96\\86\\f6\\68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "Successfully built jieba nltk\n",
      "Installing collected packages: cssselect, jieba, idna, certifi, urllib3, requests, soupsieve, beautifulsoup4, nltk, goose3\n",
      "Successfully installed beautifulsoup4-4.8.2 certifi-2019.11.28 cssselect-1.1.0 goose3-3.1.6 idna-2.9 jieba-0.42.1 nltk-3.4.5 requests-2.23.0 soupsieve-2.0 urllib3-1.25.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install goose3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from goose3 import Goose\n",
    "from collections import Counter\n",
    "from math import fabs\n",
    "import re\n",
    "\n",
    "stopWords = {\n",
    "    \"-\", \" \", \",\", \".\", \"a\", \"e\", \"i\", \"o\", \"u\", \"t\", \"about\", \"above\",\n",
    "    \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\",\n",
    "    \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\",\n",
    "    \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\",\n",
    "    \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\",\n",
    "    \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\",\n",
    "    \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\",\n",
    "    \"between\", \"beyond\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\",\n",
    "    \"cannot\", \"can't\", \"co\", \"con\", \"could\", \"couldn't\", \"de\",\n",
    "    \"describe\", \"detail\", \"did\", \"do\", \"done\", \"down\", \"due\", \"during\",\n",
    "    \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\",\n",
    "    \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\",\n",
    "    \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\",\n",
    "    \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\",\n",
    "    \"further\", \"get\", \"give\", \"go\", \"got\", \"had\", \"has\", \"hasnt\",\n",
    "    \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\",\n",
    "    \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"into\", \"is\", \"it\", \"its\", \"it's\", \"itself\", \"just\", \"keep\", \"last\",\n",
    "    \"latter\", \"latterly\", \"least\", \"less\", \"like\", \"ltd\", \"made\", \"make\",\n",
    "    \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\",\n",
    "    \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\",\n",
    "    \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\",\n",
    "    \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\",\n",
    "    \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\",\n",
    "    \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\",\n",
    "    \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"people\", \"per\",\n",
    "    \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"said\", \"same\", \"see\",\n",
    "    \"seem\", \"seemed\", \"seeming\", \"seems\", \"several\", \"she\", \"should\",\n",
    "    \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\",\n",
    "    \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\",\n",
    "    \"somewhere\", \"still\", \"such\", \"take\", \"ten\", \"than\", \"that\", \"the\",\n",
    "    \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\",\n",
    "    \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\",\n",
    "    \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\",\n",
    "    \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\",\n",
    "    \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\",\n",
    "    \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"use\", \"very\",\n",
    "    \"via\", \"want\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\",\n",
    "    \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\",\n",
    "    \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\",\n",
    "    \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\",\n",
    "    \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\",\n",
    "    \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\", \"reuters\", \"news\",\n",
    "    \"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\",\n",
    "    \"sunday\", \"mon\", \"tue\", \"wed\", \"thu\", \"fri\", \"sat\", \"sun\",\n",
    "    \"rappler\", \"rapplercom\", \"inquirer\", \"yahoo\", \"home\", \"sports\",\n",
    "    \"1\", \"10\", \"2012\", \"sa\", \"says\", \"tweet\", \"pm\", \"home\", \"homepage\",\n",
    "    \"sports\", \"section\", \"newsinfo\", \"stories\", \"story\", \"photo\",\n",
    "    \"2013\", \"na\", \"ng\", \"ang\", \"year\", \"years\", \"percent\", \"ko\", \"ako\",\n",
    "    \"yung\", \"yun\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\", \"time\",\n",
    "    \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\",\n",
    "    \"august\", \"september\", \"october\", \"november\", \"december\",\n",
    "    \"government\", \"police\"\n",
    "}\n",
    "ideal = 20.0\n",
    "\n",
    "\n",
    "def SummarizeUrl(url):\n",
    "    try:\n",
    "        article = grab_link(url)\n",
    "    except IOError:\n",
    "        print('IOError')\n",
    "        return None\n",
    "\n",
    "    if not (article and article.cleaned_text and article.title):\n",
    "        return None\n",
    "\n",
    "    return Summarize(article.title, article.cleaned_text)\n",
    "\n",
    "\n",
    "def Summarize(title, text):\n",
    "    sentences = split_sentences(text)\n",
    "    keys = keywords(text)\n",
    "    titleWords = split_words(title)\n",
    "\n",
    "    if len(sentences) <= 5:\n",
    "        return sentences\n",
    "\n",
    "    #score setences, and use the top 5 sentences\n",
    "    sentence_ranks = score(sentences, titleWords, keys)\n",
    "    return [sentence for sentence, score in sentence_ranks.most_common(5)]\n",
    "\n",
    "goose = Goose()\n",
    "def grab_link(url):\n",
    "    #extract article information using Python Goose\n",
    "    try:\n",
    "        article = goose.extract(url=url)\n",
    "        return article\n",
    "    except ValueError:\n",
    "        print('Goose failed to extract article from url')\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def score(sentences, titleWords, keywords):\n",
    "    #score sentences based on different features\n",
    "\n",
    "    senSize = len(sentences)\n",
    "    ranks = Counter()\n",
    "    for i, s in enumerate(sentences):\n",
    "        sentence = split_words(s)\n",
    "        titleFeature = title_score(titleWords, sentence)\n",
    "        sentenceLength = length_score(sentence)\n",
    "        sentencePosition = sentence_position(i+1, senSize)\n",
    "        sbsFeature = sbs(sentence, keywords)\n",
    "        dbsFeature = dbs(sentence, keywords)\n",
    "        frequency = (sbsFeature + dbsFeature) / 2.0 * 10.0\n",
    "\n",
    "        #weighted average of scores from four categories\n",
    "        totalScore = (titleFeature*1.5 + frequency*2.0 +\n",
    "                      sentenceLength*1.0 + sentencePosition*1.0) / 4.0\n",
    "        ranks[s] = totalScore\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def sbs(words, keywords):\n",
    "    score = 0.0\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    for word in words:\n",
    "        if word in keywords:\n",
    "            score += keywords[word]\n",
    "    return (1.0 / fabs(len(words)) * score)/10.0\n",
    "\n",
    "\n",
    "def dbs(words, keywords):\n",
    "    if (len(words) == 0):\n",
    "        return 0\n",
    "\n",
    "    summ = 0\n",
    "    first = []\n",
    "    second = []\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word in keywords:\n",
    "            score = keywords[word]\n",
    "            if first == []:\n",
    "                first = [i, score]\n",
    "            else:\n",
    "                second = first\n",
    "                first = [i, score]\n",
    "                dif = first[0] - second[0]\n",
    "                summ += (first[1]*second[1]) / (dif ** 2)\n",
    "\n",
    "    # number of intersections\n",
    "    k = len(set(keywords.keys()).intersection(set(words))) + 1\n",
    "    return (1/(k*(k+1.0))*summ)\n",
    "\n",
    "\n",
    "def split_words(text):\n",
    "    #split a string into array of words\n",
    "    try:\n",
    "        text = re.sub(r'[^\\w ]', '', text)  # strip special chars\n",
    "        return [x.strip('.').lower() for x in text.split()]\n",
    "    except TypeError:\n",
    "        print(\"Error while splitting characters\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def keywords(text):\n",
    "    \"\"\"get the top 10 keywords and their frequency scores\n",
    "    ignores blacklisted words in stopWords,\n",
    "    counts the number of occurrences of each word\n",
    "    \"\"\"\n",
    "    text = split_words(text)\n",
    "    numWords = len(text)  # of words before removing blacklist words\n",
    "    freq = Counter(x for x in text if x not in stopWords)\n",
    "\n",
    "    minSize = min(10, len(freq))  # get first 10\n",
    "    keywords = {x: y for x, y in freq.most_common(minSize)}  # recreate a dict\n",
    "\n",
    "    for k in keywords:\n",
    "        articleScore = keywords[k]*1.0 / numWords\n",
    "        keywords[k] = articleScore * 1.5 + 1\n",
    "\n",
    "    return keywords\n",
    "\n",
    "\n",
    "def split_sentences(text):\n",
    "    '''\n",
    "    The regular expression matches all sentence ending punctuation and splits the string at those points.\n",
    "    At this point in the code, the list looks like this [\"Hello, world\", \"!\" ... ]. The punctuation and all quotation marks\n",
    "    are separated from the actual text. The first s_iter line turns each group of two items in the list into a tuple,\n",
    "    excluding the last item in the list (the last item in the list does not need to have this performed on it). Then,\n",
    "    the second s_iter line combines each tuple in the list into a single item and removes any whitespace at the beginning\n",
    "    of the line. Now, the s_iter list is formatted correctly but it is missing the last item of the sentences list. The\n",
    "    second to last line adds this item to the s_iter list and the last line returns the full list.\n",
    "    '''\n",
    "    \n",
    "    sentences = re.split('(?<![A-ZА-ЯЁ])([.!?]\"?)(?=\\s+\\\"?[A-ZА-ЯЁ])', text)\n",
    "    s_iter = list(zip(*[iter(sentences[:-1])] * 2))\n",
    "    s_iter = [''.join(map(str,y)).lstrip() for y in s_iter]\n",
    "    s_iter.append(sentences[-1])\n",
    "    return s_iter\n",
    "\n",
    "\n",
    "\n",
    "def length_score(sentence):\n",
    "    return 1 - fabs(ideal - len(sentence)) / ideal\n",
    "\n",
    "\n",
    "def title_score(title, sentence):\n",
    "    title = [x for x in title if x not in stopWords]\n",
    "    count = 0.0\n",
    "    for word in sentence:\n",
    "        if (word not in stopWords and word in title):\n",
    "            count += 1.0\n",
    "            \n",
    "    if len(title) == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    return count/len(title)\n",
    "\n",
    "\n",
    "def sentence_position(i, size):\n",
    "    \"\"\"different sentence positions indicate different\n",
    "    probability of being an important sentence\"\"\"\n",
    "\n",
    "    normalized = i*1.0 / size\n",
    "    if 0 < normalized <= 0.1:\n",
    "        return 0.17\n",
    "    elif 0.1 < normalized <= 0.2:\n",
    "        return 0.23\n",
    "    elif 0.2 < normalized <= 0.3:\n",
    "        return 0.14\n",
    "    elif 0.3 < normalized <= 0.4:\n",
    "        return 0.08\n",
    "    elif 0.4 < normalized <= 0.5:\n",
    "        return 0.05\n",
    "    elif 0.5 < normalized <= 0.6:\n",
    "        return 0.04\n",
    "    elif 0.6 < normalized <= 0.7:\n",
    "        return 0.06\n",
    "    elif 0.7 < normalized <= 0.8:\n",
    "        return 0.04\n",
    "    elif 0.8 < normalized <= 0.9:\n",
    "        return 0.04\n",
    "    elif 0.9 < normalized <= 1.0:\n",
    "        return 0.15\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summerize single web article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Companies looking to boost their digitalisation efforts will now be able to benefit from three new programmes that collectively target workers across all levels of a business.\n",
      "\n",
      "The courses aim to equip them with the mindset and know-how needed to map out an effective digitalisation strategy for their firms.\n",
      "\n",
      "The programmes are expected to kick off early next year and are open to SBF members.\n",
      "\n",
      "Depending on each company's needs, NUS-ISS will then work with them to roll out specific training programmes for their employees.\n",
      "\n",
      "The programmes will be offered by the Singapore Business Federation (SBF) and the Institute of Systems Science at National University of Singapore (NUS-ISS), with the two organisations signing a memorandum of understanding yesterday.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NUS-ISS\n",
    "# The Straits Times: 3 new programmes to help firms boost digitalisation efforts\n",
    "url = 'https://www.straitstimes.com/tech/3-new-programmes-to-help-firms-boost-digitalisation-efforts'\n",
    "summaries = SummarizeUrl(url)\n",
    "for sentence in summaries:\n",
    "    print(sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summerize mutiple web articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url :  https://www.straitstimes.com/tech/3-new-programmes-to-help-firms-boost-digitalisation-efforts\n",
      "['Companies looking to boost their digitalisation efforts will now be able to '\n",
      " 'benefit from three new programmes that collectively target workers across '\n",
      " 'all levels of a business.',\n",
      " 'The courses aim to equip them with the mindset and know-how needed to map '\n",
      " 'out an effective digitalisation strategy for their firms.']\n",
      "\n",
      "url :  http://www.huffingtonpost.com/2013/11/22/twitter-forward-secrecy_n_4326599.html\n",
      "['\"A year and a half ago, Twitter was first served completely over HTTPS,\" the '\n",
      " 'company said in a blog posting.',\n",
      " '(Reuters) - Twitter Inc said it has implemented a security technology that '\n",
      " 'makes it harder to spy on its users and called on other Internet firms to do '\n",
      " 'the same, as Web providers look to thwart spying by government intelligence '\n",
      " 'agencies.']\n",
      "\n",
      "url :  http://www.bbc.co.uk/news/world-europe-30035666\n",
      "['Police, fire officials and a specially-trained hunting dog are searching for '\n",
      " 'a big cat - believed to be a tiger - seen in a town outside Paris.',\n",
      " 'It was unclear where the animal came from, though there is a big cat park '\n",
      " 'near Montevrain.']\n",
      "\n",
      "url :  http://www.bbc.co.uk/news/magazine-29631332\n",
      "['The collected ocean plastic would be recycled and made into products - or '\n",
      " 'oil.',\n",
      " \"Boyan Slat is a 20-year-old on a mission - to rid the world's oceans of \"\n",
      " 'floating plastic.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "urls = (u'https://www.straitstimes.com/tech/3-new-programmes-to-help-firms-boost-digitalisation-efforts',\n",
    "        u'http://www.huffingtonpost.com/2013/11/22/twitter-forward-secrecy_n_4326599.html',\n",
    "        u'http://www.bbc.co.uk/news/world-europe-30035666',\n",
    "        u'http://www.bbc.co.uk/news/magazine-29631332'        \n",
    "       )\n",
    "\n",
    "for url in urls:\n",
    "    print('url : ', url)\n",
    "    summaries = SummarizeUrl(url)\n",
    "    pprint(summaries[0:2])\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The collected ocean plastic would be recycled and made into products - or oil.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summerize local textual article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example from https://www.straitstimes.com/tech/3-new-programmes-to-help-firms-boost-digitalisation-efforts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_title = u'3 new programmes to help firms boost digitalisation efforts'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = u'Companies looking to boost their digitalisation efforts will now be able to benefit from three new programmes that collectively target workers across all levels of a business. The programmes will be offered by the Singapore Business Federation (SBF) and the Institute of Systems Science at National University of Singapore (NUS-ISS), with the two organisations signing a memorandum of understanding yesterday. Business owners and leaders can sign up for the Digital Competency Roadmap comprising three courses over five days. The courses aim to equip them with the mindset and know-how needed to map out an effective digitalisation strategy for their firms. There is also a longer five-course, 13-day variant for employees at the executive level. Executives who wish to take their classroom learning further can also work towards a Professional Diploma in Digitalisation jointly awarded by SBF and NUS-ISS. This programme will include a four-month project, in which experts from NUS-ISS will guide participants to apply what they have learnt when they are back in their own companies. The last programme, called Skills for Transformation, is targeted at companies that have already started on their digital transformations. Depending on each company\\'s needs, NUS-ISS will then work with them to roll out specific training programmes for their employees. The programmes were announced by Minister of State for National Development and Manpower Zaqy Mohamad at the Suntec Singapore Convention and Exhibition Centre during the opening ceremony of SBF\\'s Future Economy Conference and Exhibition. \"Transformation is an ongoing journey. For companies that have started, you must press on and continue to look out for new opportunities... For those that have yet to start, I encourage you to do so now,\" he said. The programmes are expected to kick off early next year and are open to SBF members. Those interested in finding out more can attend a 9.30am briefing at the SBF Centre on Nov 20 or register their interest at https://nus.edu/2oNvhfI The programmes are targeted mainly at companies from the retail, logistics, food and beverage, manufacturing and built environment sectors, but SBF is open to considering companies from other industries, said its assistant chief executive Joanne Guo. She added that SkillsFuture subsidies will offset programme costs by up to 90 per cent for small and medium-sized enterprises (SMEs) and 70 per cent for non-SMEs. Mr Zaqy also also provided an update on the Infocomm Media Development Authority\\'s Digital Acceleration Index (DAI), revealing that more than 2,000 companies have used the index since its launch earlier this year. The DAI is a self-diagnostic tool that gauges how far along companies are in their digital transformation and provides insights into their digital strengths and weaknesses. The next window for registration will open in January.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< 3 new programmes to help firms boost digitalisation efforts >>\n",
      "\n",
      "Companies looking to boost their digitalisation efforts will now be able to benefit from three new programmes that collectively target workers across all levels of a business.\n",
      "\n",
      "The courses aim to equip them with the mindset and know-how needed to map out an effective digitalisation strategy for their firms.\n",
      "\n",
      "The programmes will be offered by the Singapore Business Federation (SBF) and the Institute of Systems Science at National University of Singapore (NUS-ISS), with the two organisations signing a memorandum of understanding yesterday.\n",
      "\n",
      "The DAI is a self-diagnostic tool that gauges how far along companies are in their digital transformation and provides insights into their digital strengths and weaknesses.\n",
      "\n",
      "Depending on each company's needs, NUS-ISS will then work with them to roll out specific training programmes for their employees.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('<<', article_title, '>>\\n')\n",
    "summaries = Summarize(article_title, article_text)\n",
    "for sentence in summaries:\n",
    "    print(sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ most important sentence ]\n",
      " Companies looking to boost their digitalisation efforts will now be able to benefit from three new programmes that collectively target workers across all levels of a business.\n"
     ]
    }
   ],
   "source": [
    "print('[ most important sentence ]\\n', summaries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example from https://www.iss.nus.edu.sg/executive-education/course/detail/intelligent-process-automation/artificial-intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_title = u'NICF- Intelligent Process Automation (SF)'\n",
    "article_text = u'Overview. This 3-day course enables participants to create intelligent virtual workers (intelligent agent) able to mimic real person to take business related actions and procedures by operating a computer and computer applications, in various business processes and industry domains. Participants learn practical knowledge of intelligent agent (IA), artificial intelligence (AI), open source toolsets, and computer scripting skills to design and create intelligent virtual workers as a minimal viable product (MVP) to automate business processes. Application examples are: Virtual employee for account management, e.g. mortgage loan application, invoice processing; Interactive messaging/chat-bot for customer service; request fulfillment; frequently asked questions; Intelligent software agents in research, innovation and video game industry; This course can benefit job roles such as Automation Managers, Enterprise Architects, Application Solution Architects, System Integrator, Technology Consultants, Senior Process Analysts, Senior Process Engineers, and working professionals who intend to design, develop, implement and evaluate various applications of cognitive robotic process automation with artificial intelligence. Participants will benefit from a careful balance of lectures and practical workshops. Some of the topics covered include concepts and techniques of robotic process automation; artificial intelligence fundamentals; enterprise system integration; email automation; AI processing of image, speech, language, sentiment analysis; and interactive conversational virtual worker/assistant. There will be projects and assessment to reinforce participants’ learning as part of the course. This course is a part of Graduate Certificate in Intelligent Software Agents, which is a part of the Stackable Graduate Certificate Programme in Artificial Intelligent Systems offered by NUS-ISS. Key Takeaways. Upon completion of the course, students will be able to: Identify enterprise business use cases and applications to implement intelligent process automation (IPA), leveraging technologies of intelligent software agent and process automation. Evaluate intelligent process automation techniques, including desktop automation, web automation, visual user interface automation, spoken language and written document understanding, sentiment analysis, image text extraction, customer face detection, customer speech recognition, task agent interaction and automation through voice commands. Architect IPA applications useful to learner’s organization using tools: E.g. Virtual workers/assistants for customer care. E.g. Enterprise visual and user interface automation applications using artificial intelligence, chat-bot, and computer scripting for mortgage application automation. E.g. Reinforcement learning agent for research, innovation and video game industry. Develop IPA Minimum Viable Product (MVP). Who Should Attend. This course is suitable for both information technology and non-IT professionals who are interested in creating intelligent robots, or virtual workers, able to mimic real personnel to automatically take business related actions and procedures by operating a computer, in various business processes and industry domains. This course will be useful for: Automation Manager who want to improve personnel productivity. Enterprise Architects & Application Solution Architects who want to integrate intelligent process automation into their solution. System Integrators & Technology Consultants who want to use this technology for system integration. Senior Process Analysts & Senior Process Engineers who want to customize, configure and use virtual worker. Robotic Process Automation practitioners who want to leverage artificial intelligence technology. Prerequisites. This is a intermediate course involves rapid prototyping using several computer programing, e.g. graphical programming, JavaScript, python, APIs, and cloud computing. Participants with prior computer programming experiences would benefit more during hands-on workshops and project submission as part of course assessment. Participants should preinstall and successfully run iss-vm virtual machine (about 30 GB in size) into their own device before course start date. Participants should have basic computer literacy and software engineering fundamentals, e.g. using Windows or Linux or MacOS, Internet, Email, Spreadsheet, VMware or VirtualBox, and aware of cloud computing, application programming interface (API), and client-server software architecture. Participants should have basic hands-on coding experience in one or more high-level computer programming languages, preferable in JavaScript and Python. Basic knowledge of pattern recognition at the level of NICF- Problem Solving using Pattern Recognition (SF). Note: Participants are required to bring their own internet enabled computing device (laptops, tablet etc) & power charger to access and download electronic courseware in PDF format. We will be only be providing courseware in PDF and will not issue any printed copies.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< NICF- Intelligent Process Automation (SF) >>\n",
      "\n",
      "Evaluate intelligent process automation techniques, including desktop automation, web automation, visual user interface automation, spoken language and written document understanding, sentiment analysis, image text extraction, customer face detection, customer speech recognition, task agent interaction and automation through voice commands.\n",
      "\n",
      "Upon completion of the course, students will be able to: Identify enterprise business use cases and applications to implement intelligent process automation (IPA), leveraging technologies of intelligent software agent and process automation.\n",
      "\n",
      "Enterprise Architects & Application Solution Architects who want to integrate intelligent process automation into their solution.\n",
      "\n",
      "Some of the topics covered include concepts and techniques of robotic process automation; artificial intelligence fundamentals; enterprise system integration; email automation; AI processing of image, speech, language, sentiment analysis; and interactive conversational virtual worker/assistant.\n",
      "\n",
      "Enterprise visual and user interface automation applications using artificial intelligence, chat-bot, and computer scripting for mortgage application automation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('<<', article_title, '>>\\n')\n",
    "summaries = Summarize(article_title, article_text)\n",
    "for sentence in summaries:\n",
    "    print(sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ most important sentence ]\n",
      " Evaluate intelligent process automation techniques, including desktop automation, web automation, visual user interface automation, spoken language and written document understanding, sentiment analysis, image text extraction, customer face detection, customer speech recognition, task agent interaction and automation through voice commands.\n"
     ]
    }
   ],
   "source": [
    "print('[ most important sentence ]\\n', summaries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Your Exercise:</span> \n",
    "\n",
    "## Apply the text summarization technology to summarize textual data, e.g. emails, customer feedbacks, news articles, competitor web pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The wire bundles were not implicated in those accidents, which have been linked to problems with a software system known as MCAS.\n",
      "\n",
      "The wire bundles have raised concerns because they could, in rare circumstances, cause a short circuit and possibly lead to a catastrophic failure.\n",
      "\n",
      "Boeing has argued privately to regulators that the likelihood of such a failure is remote.\n",
      "\n",
      "Boeing had hoped to avoid having to uncover and separate the wiring, but concluded that global regulators’ insistence on it might further delay the Max’s return, the source said.\n",
      "\n",
      "The Max was grounded a year ago after it was involved in two crashes that killed a total of 346 people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.nytimes.com/2020/03/11/business/boeing-737-max-wire-bundles.html'\n",
    "summaries = SummarizeUrl(url)\n",
    "for sentence in summaries:\n",
    "    print(sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
